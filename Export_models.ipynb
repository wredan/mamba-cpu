{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to ONNX and save models notebook\n",
    "#### Requirements\n",
    "\n",
    "- install pytorch: https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required once\n",
    "%pip install --user -qqr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required\n",
    "\n",
    "from mamba_ssm.models.config_mamba import MambaConfig\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from mamba_ssm.models.model_wrapper import ModelWrapper\n",
    "\n",
    "# Config\n",
    "model_name = \"state-spaces/mamba-130m\"\n",
    "device = \"cpu\"\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below to export MambaLMHeadModel in ONNX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init model pretrained\n",
    "model = ModelWrapper(model_name=model_name, use_generation=False, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init model custom config not pretrained\n",
    "config = MambaConfig()\n",
    "config.d_model = 200\n",
    "config.n_layer = 1\n",
    "model = ModelWrapper(model_name=None, use_generation=False, config=config, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a model input\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "dummy_prompt = \"Hello, world!\"  \n",
    "tokens = tokenizer(dummy_prompt, return_tensors=\"pt\")\n",
    "input_ids = tokens.input_ids.to(device=device)\n",
    "\n",
    "onnx_model_path = \"model.onnx\"\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(model, \n",
    "                  (input_ids),  \n",
    "                  onnx_model_path,\n",
    "                  verbose=False,\n",
    "                  input_names=['input_ids'],\n",
    "                  output_names=['output'],\n",
    "                  dynamic_axes={'input_ids': {0: 'batch_size'}, 'output': {0: 'batch_size'}})\n",
    "\n",
    "torch.save(model, \"model_wrapper.pt\")\n",
    "\n",
    "print(f\"Model exported in {onnx_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model_wrapper.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to save pretrained pytorch model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MambaLMHeadModel.from_pretrained(model_name, device=device, dtype=dtype)\n",
    "torch.save(model, \"model_original_pretrained.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to save custom not pretrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MambaConfig()\n",
    "config.d_model = 200\n",
    "config.n_layer = 2\n",
    "model = MambaLMHeadModel(config=config, device=device, dtype=dtype)\n",
    "torch.save(model, \"model_custom.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
