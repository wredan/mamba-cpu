{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to ONNX and save models notebook\n",
    "#### Requirements\n",
    "\n",
    "- install pytorch: https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required once\n",
    "%pip install --user -qqr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required\n",
    "\n",
    "from mamba_ssm.models.config_mamba import MambaConfig\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from mamba_ssm.onnx.model_wrapper import ModelWrapper, BlockModelWrapper, MambaModelWrapper\n",
    "\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Config\n",
    "model_name = \"state-spaces/mamba-130m\"\n",
    "device = \"cpu\"\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a model dummy input\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "tokenizer.eos_token = \"<|endoftext|>\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dummy_prompt = \"Harry Potter\"  \n",
    "tokens = tokenizer(dummy_prompt, return_tensors=\"pt\")\n",
    "input_ids = tokens.input_ids.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Run below to export MambaLMHeadModel in ONNX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 24\n",
      "Size of d: 768\n",
      "Number of parameters: 129135360\n"
     ]
    }
   ],
   "source": [
    "# Init model pretrained\n",
    "model = ModelWrapper(model_name=model_name, use_generation=False, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 1\n",
      "Size of d: 200\n",
      "Number of parameters: 10328800\n"
     ]
    }
   ],
   "source": [
    "# Init model custom config not pretrained\n",
    "config = MambaConfig()\n",
    "config.d_model = 200\n",
    "config.n_layer = 1\n",
    "model = ModelWrapper(model_name=None, use_generation=False, config=config, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"model.onnx\"\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(model, \n",
    "                input_ids,  \n",
    "                onnx_model_path,\n",
    "                verbose=False,\n",
    "                input_names=['input_ids'],\n",
    "                output_names=['output'],\n",
    "                dynamic_axes={\n",
    "                    'input_ids': {0: 'batch_size', 1: 'seq_length'},  \n",
    "                    'output': {0: 'batch_size', 1: 'seq_length'}\n",
    "                }\n",
    ")\n",
    "\n",
    "torch.save(model, \"model_wrapper.pt\")\n",
    "\n",
    "print(f\"Model exported in {onnx_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model_wrapper.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to save pretrained pytorch model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MambaLMHeadModel.from_pretrained(model_name, device=device, dtype=dtype)\n",
    "torch.save(model, \"model_original_pretrained.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to save custom not pretrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MambaConfig()\n",
    "config.d_model = 200\n",
    "config.n_layer = 2\n",
    "model = MambaLMHeadModel(config=config, device=device, dtype=dtype)\n",
    "torch.save(model, \"model_custom.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block layer\n",
    "Run to export block layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MambaConfig()\n",
    "config.d_model = 5\n",
    "config.n_layer = 1\n",
    "block_model_wrapper = BlockModelWrapper(config=config, device=device, dtype=dtype)\n",
    "\n",
    "hidden_states = torch.randn(1, 10, config.d_model, device='cpu')  # Batch size = 1, Seq length = 10\n",
    "\n",
    "residual = torch.zeros_like(hidden_states, device=hidden_states.device, dtype=hidden_states.dtype)\n",
    "\n",
    "torch.onnx.export(\n",
    "    block_model_wrapper,\n",
    "    (input_ids, hidden_states, residual),  \n",
    "    'block_model.onnx',\n",
    "    input_names=['input_ids', 'hidden_states', 'residual'],\n",
    "    output_names=['output']\n",
    ")\n",
    "torch.save(block_model_wrapper, \"block_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mamba layer\n",
    "Run to export Mamba layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MambaConfig()\n",
    "config.d_model = 5\n",
    "config.n_layer = 1\n",
    "block_model_wrapper = MambaModelWrapper(config=config, device=device, dtype=dtype)\n",
    "\n",
    "torch.onnx.export(\n",
    "    block_model_wrapper,\n",
    "    input_ids,  \n",
    "    'mamba_model.onnx',\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input_ids'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'seq_length'},  \n",
    "        'output': {0: 'batch_size', 1: 'seq_length'}\n",
    "    }\n",
    ")\n",
    "\n",
    "torch.save(block_model_wrapper, \"mamba_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MambaConfig()\n",
    "config.d_model = 5\n",
    "config.n_layer = 1\n",
    "block_model_wrapper = MambaModelWrapper(config=config, device=device, dtype=dtype)\n",
    "block_model_wrapper.eval()\n",
    "\n",
    "# Generate a model dummy input\n",
    "dummy_prompt_1 = \"Harry test ciao\"  \n",
    "tokens_1 = tokenizer(dummy_prompt_1, return_tensors=\"pt\")\n",
    "input_ids_1 = tokens_1.input_ids.to(device=device)\n",
    "\n",
    "out = block_model_wrapper(input_ids_1)\n",
    "\n",
    "print(\"output\", out.shape)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_options = ort.SessionOptions()\n",
    "session_options.log_severity_level = 0  # 0 = VERBOSE, higher numbers indicate less verbosity\n",
    "session_options.log_verbosity_level = 5  # Adjust this for more detailed logs, higher means more verbose\n",
    "\n",
    "ort_session = ort.InferenceSession('mamba_model.onnx', sess_options=session_options)\n",
    "\n",
    "# Generate a model dummy input\n",
    "dummy_prompt_1 = \"Harry\"  \n",
    "tokens_1 = tokenizer(dummy_prompt_1, return_tensors=\"pt\")\n",
    "input_ids_1 = tokens_1.input_ids.to(device=device)\n",
    "input_ids_np = np.array(input_ids_1)\n",
    "print(input_ids_np.shape)\n",
    "\n",
    "# Inference\n",
    "inputs = {ort_session.get_inputs()[0].name: input_ids_np}\n",
    "\n",
    "out = ort_session.run(None, inputs)\n",
    "\n",
    "# Output\n",
    "print(input_ids_np.shape)\n",
    "print(np.array(out).shape)\n",
    "out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MambaConfig()\n",
    "config.d_model = 5\n",
    "config.n_layer = 1\n",
    "block_model_wrapper = MambaModelWrapper(config=config, device=device, dtype=dtype)\n",
    "\n",
    "torch.onnx.dynamo_export(\n",
    "    block_model_wrapper,\n",
    "    input_ids,\n",
    "    export_options=torch.onnx.ExportOptions(dynamic_shapes=True)\n",
    ").save(\"mamba_dyn.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
