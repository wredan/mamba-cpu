{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to ONNX and save models notebook\n",
    "#### Requirements\n",
    "\n",
    "- install pytorch: https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required once\n",
    "%pip install --user -qqr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danilo/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Required\n",
    "\n",
    "from mamba_ssm.models.config_mamba import MambaConfig\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from mamba_ssm.onnx.model_wrapper import ModelWrapper, BlockModelWrapper, MambaModelWrapper\n",
    "\n",
    "# Config\n",
    "model_name = \"state-spaces/mamba-130m\"\n",
    "device = \"cpu\"\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below to export MambaLMHeadModel in ONNX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 24\n",
      "Size of d: 768\n",
      "Number of parameters: 129135360\n"
     ]
    }
   ],
   "source": [
    "# Init model pretrained\n",
    "model = ModelWrapper(model_name=model_name, use_generation=False, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init model custom config not pretrained\n",
    "config = MambaConfig()\n",
    "config.d_model = 200\n",
    "config.n_layer = 1\n",
    "model = ModelWrapper(model_name=None, use_generation=False, config=config, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported in model.onnx\n"
     ]
    }
   ],
   "source": [
    "# Generate a model input\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "dummy_prompt = \"Hello, world!\"  \n",
    "tokens = tokenizer(dummy_prompt, return_tensors=\"pt\")\n",
    "input_ids = tokens.input_ids.to(device=device)\n",
    "\n",
    "onnx_model_path = \"model.onnx\"\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(model, \n",
    "                  (input_ids),  \n",
    "                  onnx_model_path,\n",
    "                  verbose=False,\n",
    "                  input_names=['input_ids'],\n",
    "                  output_names=['output'],\n",
    "                  dynamic_axes={'input_ids': {0: 'batch_size'}, 'output': {0: 'batch_size'}})\n",
    "\n",
    "torch.save(model, \"model_wrapper.pt\")\n",
    "\n",
    "print(f\"Model exported in {onnx_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model_wrapper.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to save pretrained pytorch model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MambaLMHeadModel.from_pretrained(model_name, device=device, dtype=dtype)\n",
    "torch.save(model, \"model_original_pretrained.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to save custom not pretrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MambaConfig()\n",
    "config.d_model = 200\n",
    "config.n_layer = 2\n",
    "model = MambaLMHeadModel(config=config, device=device, dtype=dtype)\n",
    "torch.save(model, \"model_custom.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to export block layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MambaConfig()\n",
    "config.d_model = 5\n",
    "config.n_layer = 1\n",
    "block_model_wrapper = BlockModelWrapper(config=config, device=device, dtype=dtype)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "dummy_prompt = \"Hello, world!\"  \n",
    "tokens = tokenizer(dummy_prompt, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = tokens.input_ids.to(device=device)\n",
    "hidden_states = torch.randn(1, 10, config.d_model, device='cpu')  # Batch size = 1, Seq length = 10\n",
    "\n",
    "residual = torch.zeros_like(hidden_states, device=hidden_states.device, dtype=hidden_states.dtype)\n",
    "\n",
    "torch.onnx.export(\n",
    "    block_model_wrapper,\n",
    "    (input_ids, hidden_states, residual),  \n",
    "    'block_model.onnx',\n",
    "    input_names=['input_ids', 'hidden_states', 'residual'],\n",
    "    output_names=['output']\n",
    ")\n",
    "torch.save(block_model_wrapper, \"block_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to export Mamba layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of d: 5\n",
      "Number of parameters: 720\n",
      "0 torch.Size([1, 4, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danilo/miniconda3/envs/deepl/lib/python3.11/site-packages/torch/onnx/utils.py:2095: UserWarning: Provided key hidden_states for dynamic axes is not a valid input/output name\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config = MambaConfig()\n",
    "config.d_model = 5\n",
    "config.n_layer = 1\n",
    "block_model_wrapper = MambaModelWrapper(config=config, device=device, dtype=dtype)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "dummy_prompt = \"Hello, world!\"  \n",
    "tokens = tokenizer(dummy_prompt, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = tokens.input_ids.to(device=device)\n",
    "\n",
    "torch.onnx.export(\n",
    "    block_model_wrapper,\n",
    "    input_ids,  \n",
    "    'mamba_model_d_5_h.onnx',\n",
    "    input_names=['input_ids'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'seq_length'}, \n",
    "        'hidden_states': {0: 'batch_size', 1: 'seq_length', 2: 'd_model'}, \n",
    "        'output': {0: 'batch_size', 1: 'seq_length', 2: 'd_model'}\n",
    "    }\n",
    ")\n",
    "\n",
    "torch.save(block_model_wrapper, \"mamba_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of d: 5\n",
      "Number of parameters: 720\n",
      "0 torch.Size([1, 4, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0187,  0.0326,  0.0519,  0.0323,  0.0524],\n",
       "         [-0.0337, -0.0044,  0.0254,  0.0173,  0.0112],\n",
       "         [-0.0278, -0.0198,  0.0496,  0.0301,  0.0151],\n",
       "         [ 0.0188,  0.0294,  0.0273,  0.0104, -0.0015]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = MambaConfig()\n",
    "config.d_model = 5\n",
    "config.n_layer = 1\n",
    "block_model_wrapper = MambaModelWrapper(config=config, device=device, dtype=dtype)\n",
    "block_model_wrapper.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "dummy_prompt = \"Hello, !world\"  \n",
    "tokens = tokenizer(dummy_prompt, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = tokens.input_ids.to(device=device)\n",
    "\n",
    "out = block_model_wrapper(input_ids)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
